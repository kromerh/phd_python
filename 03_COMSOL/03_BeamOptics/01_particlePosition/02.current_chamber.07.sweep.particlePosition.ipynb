{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMSOL Study: Parameter sweeps\n",
    "\n",
    "\n",
    "## Particle positions at surface of target\n",
    "\n",
    "\n",
    "- current chamber\n",
    "- microwave extraction aperture (1 mm diameter)\n",
    "- surface mesh\n",
    "- 5000 particles\n",
    "\n",
    "- COMSOL files 07.sweep. See notion card for the details of sweeps\n",
    "- only **last timestep** is imported\n",
    "\n",
    "- last accessed: 2019-02-12\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import kde\n",
    "from scipy import optimize\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib import pyplot, transforms\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify path to datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = '02.current_chamber/07.sweep/particleData/'\n",
    "\n",
    "remote_path = f'/Users/hkromer/02_PhD/02_Data/01_COMSOL/\\\n",
    "01_IonOptics/{foldername}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tophat function\n",
    "def tophat(x, base_level, hat_level, hat_mid, hat_width):\n",
    "\treturn np.where((hat_mid - hat_width / 2. < x) &\n",
    "\t\t\t\t\t(x < hat_mid + hat_width / 2.), hat_level, base_level)\n",
    "\n",
    "\n",
    "def objective(params, x, y):\n",
    "\treturn np.sum(np.abs(tophat(x, *params) - y))\n",
    "\n",
    "\n",
    "def find_center_of_spot(y, x, qry_eval):\n",
    "\t# input:\n",
    "\t#  y: gaussian fit values along y (x=0)\n",
    "\t#  x: corresponding x values\n",
    "\t#  qry_eval: query points for the vertical and horizontal line\n",
    "\n",
    "\tidx_max = np.argmax(y)\n",
    "\tx_at_max = x[idx_max]\n",
    "\n",
    "\tidx_centerline_x = (np.abs(qry_eval - x_at_max)).argmin()\n",
    "\t# print(x_at_max)\n",
    "\treturn x_at_max, idx_centerline_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweep parameter preparation\n",
    "\n",
    "Taken from notion page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sweep = {'ID': [f'00{ii}' for ii in range(1,9)], \n",
    "              'sweep_variable': ['V_HV', 'V_HV', 'Extr.offset', 'Extr.offset', 'Target.L_offset', 'Target.L_offset', 'V_extraction', 'V_extraction'],\n",
    "             'sweep_values': [[-60, -80, -100],[ -120, -150], [8, 9, 10], [11, 12, 13], [70, 80, 90], [100, 110, 120], [-2, -2.5, -3], [-3.5, -4, -4.5]]}\n",
    "\n",
    "\n",
    "df_sweep_params = pd.DataFrame(dict_sweep).set_index('ID')\n",
    "print(df_sweep_params)\n",
    "print(df_sweep_params.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which COMSOL data files are in the folder for processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMSOL_data_file_path = remote_path\n",
    "\n",
    "COMSOL_files = os.listdir(COMSOL_data_file_path)\n",
    "# COMSOL_files = [f for f in COMSOL_files if 'particleData' in f]\n",
    "COMSOL_files = [f for f in COMSOL_files if f.endswith('.csv')]\n",
    "COMSOL_files = [f'{COMSOL_data_file_path}{f}' for f in COMSOL_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through the COMSOL files loading the particle data into the main dataframe df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame()\n",
    "\n",
    "for COMSOL_data_file in COMSOL_files:\n",
    "\n",
    "    runfile = re.findall(r'[^/]+(?=/$|$)', COMSOL_data_file)[0]\n",
    "    \n",
    "    # get the file ID from runfile (to get more info on the sweep)\n",
    "    run_ID = re.findall(r'\\.(\\d\\d\\d)\\.particleData', runfile)[0]\n",
    "    \n",
    "    # select only one run_ID\n",
    "#     if run_ID == '006':\n",
    "#         pass\n",
    "#     else: \n",
    "#         continue\n",
    "    \n",
    "    print(f'Doing file {runfile} which is ID {run_ID}')\n",
    "    \n",
    "    # import the data\n",
    "    df_data = pd.read_csv(COMSOL_data_file, skiprows=8, header=None)\n",
    "    df_data['ID'] = run_ID\n",
    "    df_data.set_index('ID', inplace=True)\n",
    "    \n",
    "    # process the column\n",
    "    # find column headers\n",
    "    c = []\n",
    "    with open(COMSOL_data_file, 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            if 'Index' in line:\n",
    "                l = line.rstrip().split(',')\n",
    "                c.append(l)\n",
    "\n",
    "    myfile.close()\n",
    "    cols = c[0]\n",
    "    for ii in range(0, len(cols)):\n",
    "        c = cols[ii]\n",
    "        if 'qx' in c:\n",
    "            cols[ii] = 'qx'\n",
    "        if 'qy' in c:\n",
    "            cols[ii] = 'qy'\n",
    "        if 'qz' in c:\n",
    "            cols[ii] = 'qz'\n",
    "        if 'Index' in c:\n",
    "            cols[ii] = 'particleIndex'\n",
    "    \n",
    "    # loop over the indices 1, 3 and so on. Combine the column names\n",
    "    cols_1 = cols[1::2]\n",
    "    cols_2 = [c.strip() for c in cols[2::2]]\n",
    "    \n",
    "    cols_out = [cols[0]]\n",
    "    for ii in range(0, len(cols_1)):\n",
    "        cols_out.append(cols_1[ii] + '_' + cols_2[ii])\n",
    "   \n",
    "    # set the new column names\n",
    "    df_data.columns = cols_out\n",
    "\n",
    "    # distinct column names in cols_2 is the different sweep settings\n",
    "    cols_dist = list(set(cols_2))\n",
    "    \n",
    "    # loop over each sweep_setting\n",
    "    for sweep_val in cols_dist:\n",
    "        cols_to_select = [item for item in df_data.columns if item.endswith(sweep_val)]\n",
    "        df_data_sweep = df_data.loc[:, cols_to_select]\n",
    "#         print(df_data_sweep.head())\n",
    "        print(' ')\n",
    "        print(f'***Now doing {sweep_val}')\n",
    "        print(' ') \n",
    "        # length: total number of particles\n",
    "        n_total = len(df_data_sweep)\n",
    "\n",
    "        # only those particles that have made it to the target: 10 mm in +x direction\n",
    "        # for dist in [1,5,10,80]:\n",
    "        dist_min = 10.0\n",
    "        dist_max = 140.0  # needs to be adjusted for the 10.run\n",
    "        df_arrived = df_data_sweep[ (df_data_sweep.iloc[:,0] > dist_min) &(df_data_sweep.iloc[:,0] < dist_max) ]\n",
    "        n_arrived = len(df_arrived)\n",
    "\n",
    "        # percent of those that have arrived\n",
    "        perc_arrived = round((n_arrived / n_total)*100.0,2)\n",
    "\n",
    "        print('{}% of the initial {} particles have arrived at the target (x > {} mm and x < {} mm).'.format(perc_arrived, n_total, dist_min, dist_max))\n",
    "        if perc_arrived < 10:\n",
    "            print('{}% smaller than 10 %, avoid this file).'.format(perc_arrived))\n",
    "            continue\n",
    "        # print(df.head())\n",
    "        # print(sys.exit())\n",
    "\n",
    "        cols = df_arrived.columns\n",
    "\n",
    "        assert 'qx' in cols[0]\n",
    "        assert 'qy' in cols[1]\n",
    "        assert 'qz' in cols[2]\n",
    "#         print(df_arrived)\n",
    "\n",
    "        df_arrived.columns = ['qx', 'qy', 'qz']\n",
    "\n",
    "        df_arrived.plot(kind='box', y='qx')\n",
    "        plt.title('Particle position x for last timestep')\n",
    "        plt.ylabel('x [mm]')\n",
    "        df_arrived.plot(kind='box', y='qy')\n",
    "        plt.title('Particle position y for last timestep')\n",
    "        plt.ylabel('y [mm]')\n",
    "        df_arrived.plot(kind='box', y='qz')\n",
    "        plt.title('Particle position z for last timestep')\n",
    "        plt.ylabel('z [mm]')\n",
    "        plt.show()\n",
    "\n",
    "        print(f'******Doing plots...')\n",
    "        # compute beam radius\n",
    "        this_df = df_data_sweep\n",
    "\n",
    "        qy = this_df.iloc[:,1]\n",
    "        qz = this_df.iloc[:,2]\n",
    "\n",
    "        fname = re.findall(r'/([\\w.]+).csv',COMSOL_data_file)[0]\n",
    "        # print(fname)\n",
    "        directory = '{}/plots/2D_histograms_lastTimestep'.format(COMSOL_data_file_path,fname)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        qx = this_df.iloc[:,0]\n",
    "        median_qx = np.median(qx)\n",
    "        qy = this_df.iloc[:,1]\n",
    "        qz = this_df.iloc[:,2]\n",
    "        qr = np.sqrt(qy**2+qz**2)\n",
    "\n",
    "        nbins = 200\n",
    "        if run_ID == '006':\n",
    "            lim = 6\n",
    "        else:\n",
    "            lim = 3\n",
    "        x = qy\n",
    "        y = qz\n",
    "        data = np.vstack([qy, qz])\n",
    "        k = kde.gaussian_kde(data)\n",
    "        # xi, yi = np.mgrid[x.min():x.max():nbins*1j, y.min():y.max():nbins*1j]\n",
    "        xi, yi = np.mgrid[-lim:lim:nbins*1j, -lim:lim:nbins*1j]\n",
    "        zi = k(np.vstack([xi.flatten(), yi.flatten()]))\n",
    "        df_histData = pd.DataFrame()\n",
    "\n",
    "        df_histData['qx'] = qx\n",
    "        df_histData['qy'] = qy\n",
    "        df_histData['qz'] = qz\n",
    "\n",
    "        # compute FWHM for all points parallel to the x and y axis\n",
    "        qry_eval = np.linspace(-lim, lim, 100)\n",
    "        y_values = [k.evaluate([0, y])[0] for y in qry_eval]\n",
    "\n",
    "        eval_y = y_values\n",
    "\n",
    "\n",
    "        # print(kint)\n",
    "        df_res = pd.DataFrame()\n",
    "        df_res['qry_eval'] = qry_eval\n",
    "        df_res['eval_y'] = eval_y\n",
    "\n",
    "        # fit FWHM\n",
    "        # Create a function which returns a Gaussian (normal) distribution.\n",
    "        def gauss(p, x):\n",
    "            a, b, c, d = p\n",
    "            y = a*np.exp(-np.power((x - b), 2.)/(2. * c**2.)) + d\n",
    "            return y\n",
    "        def errfunc(p, x, y):\n",
    "            return gauss(p, x) - y # Distance to the fit function\n",
    "\n",
    "        p0 = [1, 1, 1, 1] # Initial guess for the parameters\n",
    "\n",
    "        # fit for parallel to y axis\n",
    "        X_f = qry_eval\n",
    "        Y_f = eval_y\n",
    "        # print(df.norm_cps)\n",
    "        # print(X_f, Y_f)\n",
    "        p1, success = optimize.leastsq(errfunc, p0[:], args=(X_f, Y_f))\n",
    "        Y_fit = gauss(p1, X_f)\n",
    "        # save df to csv\n",
    "        df_FWHM_y = pd.DataFrame(X_f, columns=['X_fit'])\n",
    "        df_FWHM_y['Y_fit'] = Y_fit\n",
    "        df_FWHM_y['sigma'] = p1[2]  # sigma in gaussian\n",
    "        df_FWHM_y['FWHM'] = 2.08 * p1[2] * np.sqrt(2 * np.log(2))  # FWHM\n",
    "        # fname = f'{master_folder}/df_FWHM_y.csv'\n",
    "        # df_FWHM_y.to_csv(fname)\n",
    "        df_FWHM['FWHM_y'] = df_FWHM_y['FWHM'].unique()\n",
    "\n",
    "\n",
    "        # find maximum position of gaussian fit along y\n",
    "        # to find center of 03_BeamOptics\n",
    "        val_x, qry_x = find_center_of_spot(df_FWHM_y['Y_fit'].values,\n",
    "                                    df_FWHM_y['X_fit'].values,\n",
    "                                    qry_eval)\n",
    "\n",
    "        # print(qry_x)\n",
    "        # sys.exit()\n",
    "        # qry_eval = np.linspace(-lim, lim, 100)\n",
    "        eval_x = [k.evaluate([x, val_x])[0] for x in qry_eval]\n",
    "        # print(eval_x)\n",
    "        df_res['eval_x'] = eval_x\n",
    "        # fit for parallel to x axis\n",
    "        X_f = qry_eval\n",
    "        Y_f = eval_x\n",
    "        # print(df.norm_cps)\n",
    "        # print(X_f, Y_f)\n",
    "        p1, success = optimize.leastsq(errfunc, p0[:], args=(X_f, Y_f), maxfev=100000)\n",
    "        Y_fit = gauss(p1, X_f)\n",
    "        # save df to csv\n",
    "        df_FWHM_x = pd.DataFrame(X_f, columns=['X_fit'])\n",
    "        df_FWHM_x['Y_fit'] = Y_fit\n",
    "        df_FWHM_x['sigma'] = p1[2]  # sigma in gaussian\n",
    "        df_FWHM_x['FWHM'] = 2.08 * p1[2] * np.sqrt(2 * np.log(2))  # FWHM\n",
    "        # fname = f'{master_folder}/df_FWHM_x.csv'\n",
    "        # df_FWHM_x.to_csv(fname)\n",
    "        df_FWHM['FWHM_x'] = df_FWHM_x['FWHM'].unique()\n",
    "\n",
    "        df_FWHM['FWHM'] = (np.abs(df_FWHM['FWHM_y']) +\n",
    "                            np.abs(df_FWHM['FWHM_x'])) / 2.0\n",
    "\n",
    "        # plt.plot(X_f, eval_x)\n",
    "        # plt.show()\n",
    "        # sys.exit()\n",
    "\n",
    "        # tophat\n",
    "        tophat_params = [ [0, 0, 0, 0], [0, 0, 0, 0] ]  # x, y\n",
    "        for eval, mode in zip([eval_x, eval_y], ['hat_x_width', 'hat_y_width']):\n",
    "            guess = [0, 0.3, 0, 2] # (base_level, hat_level, hat_mid, hat_width)\n",
    "            res = minimize(objective, guess, args=(X_f, eval),\n",
    "                method='Nelder-Mead',\n",
    "                options={'maxfev': 10000000})\n",
    "            # plt.plot(X_f, tophat(X_f, *(res.x)))\n",
    "            df_FWHM[mode] = res.x[3]\n",
    "            if mode == 'hat_x_width':\n",
    "                tophat_params[0] = res.x\n",
    "            else:\n",
    "                tophat_params[1] = res.x\n",
    "\n",
    "\n",
    "\n",
    "        f = plt.figure(1, figsize=(7.5, 7.5))\n",
    "\n",
    "        nullfmt = NullFormatter()         # no labels\n",
    "\n",
    "        # definitions for the axes\n",
    "        left, width = 0.10, 0.6\n",
    "        bottom, height = 0.10, 0.6\n",
    "        bottom_h = left_h = left + width + 0.02\n",
    "\n",
    "        rect_scatter = [left, bottom, width, height]\n",
    "        rect_histx = [left, bottom_h, width, 0.2]\n",
    "        rect_histy = [left_h, bottom, 0.2, height]\n",
    "\n",
    "        axScatter = plt.axes(rect_scatter)\n",
    "\n",
    "        axHistx = plt.axes(rect_histx)\n",
    "        plt.title(f'{runfile}- {sweep_val}')\n",
    "        axHisty = plt.axes(rect_histy)\n",
    "\n",
    "        # no labels\n",
    "        axHistx.xaxis.set_major_formatter(nullfmt)\n",
    "        axHisty.yaxis.set_major_formatter(nullfmt)\n",
    "        axHistx.grid(True)\n",
    "        axHisty.grid(True)\n",
    "\n",
    "\n",
    "        p = axScatter.pcolormesh(xi, yi, zi.reshape(xi.shape), shading='gouraud', cmap=plt.cm.jet)\n",
    "        axScatter.set_xlabel('y [mm]')\n",
    "        axScatter.set_ylabel('z [mm]')\n",
    "\n",
    "        # contours = axScatter.contour(xi, yi, zi.reshape(xi.shape), cmap=plt.cm.Blues, levels=[my_lvl])\n",
    "        # plt.clabel(contours, inline=True, fontsize=8)\n",
    "        axScatter.set_facecolor('#000080ff')\n",
    "        plt.colorbar(p)\n",
    "\n",
    "        # compute FWHM for all x and y histograms\n",
    "        # select the largest FWHM\n",
    "\n",
    "        axScatter.set_xlim((-lim, lim))\n",
    "        axScatter.set_ylim((-lim, lim))\n",
    "\n",
    "        # left = 0.8\n",
    "        # bottom = 0.12\n",
    "        # width = 0.05\n",
    "        # height = 0.65\n",
    "        #\n",
    "        # cax = f.add_axes([left, bottom, width, height])\n",
    "        # cbar = f.colorbar(p, cax)\n",
    "        #\n",
    "        # cbar.ax.tick_params(labelsize=12)\n",
    "        #\n",
    "        # plt.subplots_adjust(left=None, bottom=0.2, right=None, top=0.68,\n",
    "        # wspace=None, hspace=0.2)\n",
    "\n",
    "        # compute FWHM for all points parallel to the x and y axis\n",
    "        # qry_eval = np.linspace(-lim,lim,100)\n",
    "        # eval_x = [k.evaluate([x,0])[0] for x in qry_eval]\n",
    "        # eval_y = [k.evaluate([0,y])[0] for y in qry_eval]\n",
    "\n",
    "        # # print(kint)\n",
    "        # #\n",
    "        # df_res = pd.DataFrame()\n",
    "        # df_res['qry_eval'] = qry_eval\n",
    "        # df_res['eval_x'] = eval_x\n",
    "        # df_res['eval_y'] = eval_y\n",
    "        # # df_res['kint_1'] = kint\n",
    "        # df_res['type_file'] = type_file\n",
    "        # # df_res['contour_level'] = my_lvl\n",
    "        # fwhm_x = calculateFWHM(qry_eval,df_res['eval_x'])\n",
    "        # fwhm_y = calculateFWHM(qry_eval,df_res['eval_y'])\n",
    "\n",
    "\n",
    "        # first of all, the base transformation of the data points is needed\n",
    "        base = pyplot.gca().transData\n",
    "        rot = transforms.Affine2D().rotate_deg(270)\n",
    "        axScatter.plot([-lim, lim], [val_x, val_x], color='black', linestyle='dashed')\n",
    "        axScatter.plot([0, 0], [-lim, lim], color='black')\n",
    "        plt.axis('equal')\n",
    "        # print(tophat_params)\n",
    "        axHistx.plot(df_res['qry_eval'].values, df_res['eval_x'], c='black', linestyle='dashed')\n",
    "        axHistx.plot(df_FWHM_x['X_fit'], df_FWHM_x['Y_fit'], c='red', linestyle='dotted')\n",
    "        axHistx.plot(df_FWHM_x['X_fit'], tophat(df_FWHM_x['X_fit'], *(tophat_params[0])), c='green', linestyle='dotted')\n",
    "        axHisty.plot(df_res['qry_eval'].values, df_res['eval_y'].values[::-1], c='black', transform= rot + base)\n",
    "        axHisty.plot(df_FWHM_y['X_fit'], df_FWHM_y['Y_fit'].values[::-1], c='red', linestyle='dotted', transform= rot + base)\n",
    "        # axHisty.plot(df_FWHM_y['X_fit'], tophat(df_FWHM_y['X_fit'], *(tophat_params[1]))[::-1], c='green', linestyle='dotted', transform= rot + base)\n",
    "\n",
    "\n",
    "        axHistx.set_xlim(axScatter.get_xlim())\n",
    "        axHisty.set_ylim(axScatter.get_ylim())\n",
    "\n",
    "        # f.tight_layout()\n",
    "        # df_x_FWHM, df_y_FWHM = getLargestFWHM(k, lim)\n",
    "\n",
    "        # df_x_FWHM.to_csv('{}/df_x_FWHM.csv'.format(directory))\n",
    "        # df_y_FWHM.to_csv('{}/df_y_FWHM.csv'.format(directory))\n",
    "\n",
    "        # plt.show()\n",
    "\n",
    "        filename =  '{}/{}_{}'.format(directory, runfile, sweep_val)\n",
    "        df_histData.to_csv(f'{filename}_df_histData.csv')\n",
    "        # print(nn)\n",
    "        # plt.savefig(filename + '.eps', dpi=1200)\n",
    "        # plt.savefig(filename + '.svg', dpi=1200)\n",
    "        plt.savefig(filename + '.png', dpi=600)\n",
    "\n",
    "        plt.close('all')\n",
    "        if 'BIDIR' in runfile:\n",
    "            s_type = 'BIDIR'\n",
    "        else:\n",
    "            s_type = 'TD'\n",
    "        # id = re.findall(r'(\\d\\d)\\.', runfile)[0]\n",
    "        df_FWHM['id'] = runfile\n",
    "        df_FWHM['runfile'] = runfile\n",
    "        df_FWHM['run_type'] = s_type\n",
    "        df_FWHM['sweep_variable'] = sweep_val\n",
    "        df_output = df_output.append(df_FWHM)\n",
    "        # plt.show()\n",
    "\n",
    "fname = f'{COMSOL_data_file_path}/plots/2D_histograms_lastTimestep/df_FWHMs.csv'\n",
    "df_output = df_output.set_index('id').sort_index()\n",
    "df_output.to_csv(fname)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
