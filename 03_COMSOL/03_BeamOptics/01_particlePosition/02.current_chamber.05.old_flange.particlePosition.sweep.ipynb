{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMSOL Study: Check for simulation of old flange (surface defined mesh)\n",
    "## Particle positions at surface of target\n",
    "- current chamber\n",
    "- COMSOL files 05.old_flange - current vacuum chamber with microwave ion source (1 mm aperture)\n",
    "\n",
    "- last accessed: 2019-02-08\n",
    "\n",
    "- Hypothesis\n",
    "\n",
    "\n",
    "1. There is a difference between the neutron emitting spot sizes for the two ion sources, corresponding to different electric fields (Cu flange, distances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import kde\n",
    "from scipy import optimize\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib import pyplot, transforms\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify path to datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = '02.current_chamber/05.old_flange/particleData/'\n",
    "\n",
    "remote_path = f'/Users/hkromer/02_PhD/02_Data/01_COMSOL/\\\n",
    "01_IonOptics/{foldername}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tophat function\n",
    "def tophat(x, base_level, hat_level, hat_mid, hat_width):\n",
    "\treturn np.where((hat_mid - hat_width / 2. < x) &\n",
    "\t\t\t\t\t(x < hat_mid + hat_width / 2.), hat_level, base_level)\n",
    "\n",
    "\n",
    "def objective(params, x, y):\n",
    "\treturn np.sum(np.abs(tophat(x, *params) - y))\n",
    "\n",
    "\n",
    "def find_center_of_spot(y, x, qry_eval):\n",
    "\t# input:\n",
    "\t#  y: gaussian fit values along y (x=0)\n",
    "\t#  x: corresponding x values\n",
    "\t#  qry_eval: query points for the vertical and horizontal line\n",
    "\n",
    "\tidx_max = np.argmax(y)\n",
    "\tx_at_max = x[idx_max]\n",
    "\n",
    "\tidx_centerline_x = (np.abs(qry_eval - x_at_max)).argmin()\n",
    "\t# print(x_at_max)\n",
    "\treturn x_at_max, idx_centerline_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing file 05.old_flange.004.particleData.csv\n",
      "0.0% of the initial 1000 particles have arrived at the target (x > 10.0 mm and x < 90.0 mm).\n",
      "0.0% smaller than 10 %, avoid this file).\n",
      "Doing file 05.old_flange.002.particleData.txt.csv\n",
      "100.0% of the initial 1000 particles have arrived at the target (x > 10.0 mm and x < 90.0 mm).\n",
      "Creating plot for time time t=2.5E-7 s\n",
      "Doing file 05.old_flange.005.particleData.csv\n",
      "100.0% of the initial 1000 particles have arrived at the target (x > 10.0 mm and x < 90.0 mm).\n",
      "Creating plot for time time t=2.5E-7 s\n",
      "Doing file 05.old_flange.001.particleData.csv\n",
      "100.0% of the initial 1000 particles have arrived at the target (x > 10.0 mm and x < 90.0 mm).\n",
      "Creating plot for time time t=2.5E-7 s\n",
      "Doing file 05.old_flange.006.particleData.csv\n",
      "100.0% of the initial 1000 particles have arrived at the target (x > 10.0 mm and x < 90.0 mm).\n",
      "Creating plot for time time t=2.5E-7 s\n"
     ]
    }
   ],
   "source": [
    "COMSOL_data_file_path = remote_path\n",
    "\n",
    "COMSOL_files = os.listdir(COMSOL_data_file_path)\n",
    "# COMSOL_files = [f for f in COMSOL_files if 'particleData' in f]\n",
    "COMSOL_files = [f for f in COMSOL_files if f.endswith('.csv')]\n",
    "COMSOL_files = [f'{COMSOL_data_file_path}{f}' for f in COMSOL_files]\n",
    "\n",
    "df_output = pd.DataFrame()\n",
    "for COMSOL_data_file in COMSOL_files:\n",
    "\n",
    "    runfile = re.findall(r'[^/]+(?=/$|$)', COMSOL_data_file)[0]\n",
    "    print(f'Doing file {runfile}')\n",
    "    # import the particle data file\n",
    "    df = pd.read_csv(COMSOL_data_file, skiprows=8, header=None)\n",
    "    df_FWHM = pd.DataFrame()  # this is the output dataframe that contains the FWHM\n",
    "    # find column headers\n",
    "    c = []\n",
    "    with open(COMSOL_data_file, 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            if 'Index' in line:\n",
    "                l = line.rstrip().split(',')\n",
    "                c.append(l)\n",
    "\n",
    "    myfile.close()\n",
    "    cols = c[0]\n",
    "    # print(c)\n",
    "    # get the time stepping\n",
    "    # extract t=... from the cols\n",
    "    my_cols = []\n",
    "    for ii in range(0,len(cols)):\n",
    "        col = cols[ii]\n",
    "\n",
    "        t0 = re.findall(r'(t=.*)', col)\n",
    "        if len(t0) > 0:\n",
    "            my_cols.append(t0[0])\n",
    "        else:\n",
    "            my_cols.append('noTimestamp')\n",
    "\n",
    "    # timeStep = my_cols[4]\n",
    "    time_cols = (pd.Series(item for item in my_cols)).unique()[1:] # drop the timestamp\n",
    "\n",
    "    #set column header of df\n",
    "    cols[0] = 'particleindex'\n",
    "    df.columns = cols\n",
    "\n",
    "    # check which particles have arrived at the target\n",
    "    # get the latest timestamp\n",
    "    df_last = df.filter(regex=time_cols[-1], axis=1)\n",
    "\n",
    "    # length: total number of particles\n",
    "    n_total = len(df_last)\n",
    "\n",
    "    # only those particles that have made it to the target: 10 mm in +x direction\n",
    "    # for dist in [1,5,10,80]:\n",
    "    dist_min = 10.0\n",
    "    dist_max = 90.0  # needs to be adjusted for the 10.run\n",
    "    df_arrived = df_last[ (df_last.iloc[:,0] > dist_min) &(df_last.iloc[:,0] < dist_max) ]\n",
    "    n_arrived = len(df_arrived)\n",
    "    # print(df_last)\n",
    "    # print(df_last)\n",
    "    # percent of those that have arrived\n",
    "    perc_arrived = round((n_arrived / n_total)*100.0,2)\n",
    "\n",
    "    print('{}% of the initial {} particles have arrived at the target (x > {} mm and x < {} mm).'.format(perc_arrived, n_total, dist_min, dist_max))\n",
    "    if perc_arrived < 10:\n",
    "        print('{}% smaller than 10 %, avoid this file).'.format(perc_arrived))\n",
    "        continue\n",
    "    # print(df.head())\n",
    "    # print(sys.exit())\n",
    "    # index of the particle that have arrived at the target\n",
    "    idx_arrived = df_arrived.index.tolist()\n",
    "\n",
    "\n",
    "    # select only the last timestep\n",
    "    this_df = df.filter(regex=time_cols[-1], axis=1)\n",
    "#     print(this_df)\n",
    "#     sys.exit()\n",
    "    # compute beam radius\n",
    "\n",
    "    this_df = this_df.iloc[idx_arrived,:]\n",
    "\n",
    "    qy = this_df.iloc[:,1]\n",
    "    qz = this_df.iloc[:,2]\n",
    "\n",
    "    fname = re.findall(r'/([\\w.]+).csv',COMSOL_data_file)[0]\n",
    "    # print(fname)\n",
    "    directory = '{}/plots/2D_histograms_lastTimestep'.format(COMSOL_data_file_path,fname)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "    mytime = time_cols[-1]\n",
    "    print('Creating plot for time time {} s'.format(mytime))\n",
    "    # select only the last timestep\n",
    "    this_df = df.filter(regex=mytime, axis=1)\n",
    "\n",
    "    # compute beam radius\n",
    "\n",
    "    this_df = this_df.iloc[idx_arrived,:]\n",
    "    qx = this_df.iloc[:,0]\n",
    "    median_qx = np.median(qx)\n",
    "    qy = this_df.iloc[:,1]\n",
    "    qz = this_df.iloc[:,2]\n",
    "    qr = np.sqrt(qy**2+qz**2)\n",
    "\n",
    "    nbins = 200\n",
    "    lim = 3\n",
    "    x = qy\n",
    "    y = qz\n",
    "    data = np.vstack([qy, qz])\n",
    "    k = kde.gaussian_kde(data)\n",
    "    # xi, yi = np.mgrid[x.min():x.max():nbins*1j, y.min():y.max():nbins*1j]\n",
    "    xi, yi = np.mgrid[-3:3:nbins*1j, -3:3:nbins*1j]\n",
    "    zi = k(np.vstack([xi.flatten(), yi.flatten()]))\n",
    "    df_histData = pd.DataFrame()\n",
    "\n",
    "    df_histData['qx'] = qx\n",
    "    df_histData['qy'] = qy\n",
    "    df_histData['qz'] = qz\n",
    "\n",
    "    # compute FWHM for all points parallel to the x and y axis\n",
    "    qry_eval = np.linspace(-lim, lim, 100)\n",
    "    y_values = [k.evaluate([0, y])[0] for y in qry_eval]\n",
    "\n",
    "    eval_y = y_values\n",
    "\n",
    "\n",
    "    # print(kint)\n",
    "    df_res = pd.DataFrame()\n",
    "    df_res['qry_eval'] = qry_eval\n",
    "    df_res['eval_y'] = eval_y\n",
    "\n",
    "    # fit FWHM\n",
    "    # Create a function which returns a Gaussian (normal) distribution.\n",
    "    def gauss(p, x):\n",
    "        a, b, c, d = p\n",
    "        y = a*np.exp(-np.power((x - b), 2.)/(2. * c**2.)) + d\n",
    "        return y\n",
    "    def errfunc(p, x, y):\n",
    "        return gauss(p, x) - y # Distance to the fit function\n",
    "\n",
    "    p0 = [1, 1, 1, 1] # Initial guess for the parameters\n",
    "\n",
    "    # fit for parallel to y axis\n",
    "    X_f = qry_eval\n",
    "    Y_f = eval_y\n",
    "    # print(df.norm_cps)\n",
    "    # print(X_f, Y_f)\n",
    "    p1, success = optimize.leastsq(errfunc, p0[:], args=(X_f, Y_f))\n",
    "    Y_fit = gauss(p1, X_f)\n",
    "    # save df to csv\n",
    "    df_FWHM_y = pd.DataFrame(X_f, columns=['X_fit'])\n",
    "    df_FWHM_y['Y_fit'] = Y_fit\n",
    "    df_FWHM_y['sigma'] = p1[2]  # sigma in gaussian\n",
    "    df_FWHM_y['FWHM'] = 2.08 * p1[2] * np.sqrt(2 * np.log(2))  # FWHM\n",
    "    # fname = f'{master_folder}/df_FWHM_y.csv'\n",
    "    # df_FWHM_y.to_csv(fname)\n",
    "    df_FWHM['FWHM_y'] = df_FWHM_y['FWHM'].unique()\n",
    "\n",
    "\n",
    "    # find maximum position of gaussian fit along y\n",
    "    # to find center of 03_BeamOptics\n",
    "    val_x, qry_x = find_center_of_spot(df_FWHM_y['Y_fit'].values,\n",
    "                                df_FWHM_y['X_fit'].values,\n",
    "                                qry_eval)\n",
    "\n",
    "    # print(qry_x)\n",
    "    # sys.exit()\n",
    "    # qry_eval = np.linspace(-lim, lim, 100)\n",
    "    eval_x = [k.evaluate([x, val_x])[0] for x in qry_eval]\n",
    "    # print(eval_x)\n",
    "    df_res['eval_x'] = eval_x\n",
    "    # fit for parallel to x axis\n",
    "    X_f = qry_eval\n",
    "    Y_f = eval_x\n",
    "    # print(df.norm_cps)\n",
    "    # print(X_f, Y_f)\n",
    "    p1, success = optimize.leastsq(errfunc, p0[:], args=(X_f, Y_f), maxfev=100000)\n",
    "    Y_fit = gauss(p1, X_f)\n",
    "    # save df to csv\n",
    "    df_FWHM_x = pd.DataFrame(X_f, columns=['X_fit'])\n",
    "    df_FWHM_x['Y_fit'] = Y_fit\n",
    "    df_FWHM_x['sigma'] = p1[2]  # sigma in gaussian\n",
    "    df_FWHM_x['FWHM'] = 2.08 * p1[2] * np.sqrt(2 * np.log(2))  # FWHM\n",
    "    # fname = f'{master_folder}/df_FWHM_x.csv'\n",
    "    # df_FWHM_x.to_csv(fname)\n",
    "    df_FWHM['FWHM_x'] = df_FWHM_x['FWHM'].unique()\n",
    "\n",
    "    df_FWHM['FWHM'] = (np.abs(df_FWHM['FWHM_y']) +\n",
    "                        np.abs(df_FWHM['FWHM_x'])) / 2.0\n",
    "\n",
    "    # plt.plot(X_f, eval_x)\n",
    "    # plt.show()\n",
    "    # sys.exit()\n",
    "\n",
    "    # tophat\n",
    "    tophat_params = [ [0, 0, 0, 0], [0, 0, 0, 0] ]  # x, y\n",
    "    for eval, mode in zip([eval_x, eval_y], ['hat_x_width', 'hat_y_width']):\n",
    "        guess = [0, 0.3, 0, 2] # (base_level, hat_level, hat_mid, hat_width)\n",
    "        res = minimize(objective, guess, args=(X_f, eval),\n",
    "            method='Nelder-Mead',\n",
    "            options={'maxfev': 10000000})\n",
    "        # plt.plot(X_f, tophat(X_f, *(res.x)))\n",
    "        df_FWHM[mode] = res.x[3]\n",
    "        if mode == 'hat_x_width':\n",
    "            tophat_params[0] = res.x\n",
    "        else:\n",
    "            tophat_params[1] = res.x\n",
    "\n",
    "\n",
    "\n",
    "    f = plt.figure(1, figsize=(7.5, 7.5))\n",
    "\n",
    "    nullfmt = NullFormatter()         # no labels\n",
    "\n",
    "    # definitions for the axes\n",
    "    left, width = 0.10, 0.6\n",
    "    bottom, height = 0.10, 0.6\n",
    "    bottom_h = left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.2]\n",
    "    rect_histy = [left_h, bottom, 0.2, height]\n",
    "\n",
    "    axScatter = plt.axes(rect_scatter)\n",
    "\n",
    "    axHistx = plt.axes(rect_histx)\n",
    "    plt.title(f'{runfile}')\n",
    "    axHisty = plt.axes(rect_histy)\n",
    "\n",
    "    # no labels\n",
    "    axHistx.xaxis.set_major_formatter(nullfmt)\n",
    "    axHisty.yaxis.set_major_formatter(nullfmt)\n",
    "    axHistx.grid(True)\n",
    "    axHisty.grid(True)\n",
    "\n",
    "\n",
    "    p = axScatter.pcolormesh(xi, yi, zi.reshape(xi.shape), shading='gouraud', cmap=plt.cm.jet)\n",
    "    axScatter.set_xlabel('y [mm]')\n",
    "    axScatter.set_ylabel('z [mm]')\n",
    "\n",
    "    # contours = axScatter.contour(xi, yi, zi.reshape(xi.shape), cmap=plt.cm.Blues, levels=[my_lvl])\n",
    "    # plt.clabel(contours, inline=True, fontsize=8)\n",
    "    axScatter.set_facecolor('#000080ff')\n",
    "    plt.colorbar(p)\n",
    "\n",
    "    # compute FWHM for all x and y histograms\n",
    "    # select the largest FWHM\n",
    "\n",
    "    axScatter.set_xlim((-lim, lim))\n",
    "    axScatter.set_ylim((-lim, lim))\n",
    "\n",
    "    # left = 0.8\n",
    "    # bottom = 0.12\n",
    "    # width = 0.05\n",
    "    # height = 0.65\n",
    "    #\n",
    "    # cax = f.add_axes([left, bottom, width, height])\n",
    "    # cbar = f.colorbar(p, cax)\n",
    "    #\n",
    "    # cbar.ax.tick_params(labelsize=12)\n",
    "    #\n",
    "    # plt.subplots_adjust(left=None, bottom=0.2, right=None, top=0.68,\n",
    "    # \twspace=None, hspace=0.2)\n",
    "\n",
    "    # compute FWHM for all points parallel to the x and y axis\n",
    "    # qry_eval = np.linspace(-lim,lim,100)\n",
    "    # eval_x = [k.evaluate([x,0])[0] for x in qry_eval]\n",
    "    # eval_y = [k.evaluate([0,y])[0] for y in qry_eval]\n",
    "\n",
    "    # # print(kint)\n",
    "    # #\n",
    "    # df_res = pd.DataFrame()\n",
    "    # df_res['qry_eval'] = qry_eval\n",
    "    # df_res['eval_x'] = eval_x\n",
    "    # df_res['eval_y'] = eval_y\n",
    "    # # df_res['kint_1'] = kint\n",
    "    # df_res['type_file'] = type_file\n",
    "    # # df_res['contour_level'] = my_lvl\n",
    "    # fwhm_x = calculateFWHM(qry_eval,df_res['eval_x'])\n",
    "    # fwhm_y = calculateFWHM(qry_eval,df_res['eval_y'])\n",
    "\n",
    "\n",
    "    # first of all, the base transformation of the data points is needed\n",
    "    base = pyplot.gca().transData\n",
    "    rot = transforms.Affine2D().rotate_deg(270)\n",
    "    axScatter.plot([-lim, lim], [val_x, val_x], color='black', linestyle='dashed')\n",
    "    axScatter.plot([0, 0], [-lim, lim], color='black')\n",
    "    plt.axis('equal')\n",
    "    # print(tophat_params)\n",
    "    axHistx.plot(df_res['qry_eval'].values, df_res['eval_x'], c='black', linestyle='dashed')\n",
    "    axHistx.plot(df_FWHM_x['X_fit'], df_FWHM_x['Y_fit'], c='red', linestyle='dotted')\n",
    "    axHistx.plot(df_FWHM_x['X_fit'], tophat(df_FWHM_x['X_fit'], *(tophat_params[0])), c='green', linestyle='dotted')\n",
    "    axHisty.plot(df_res['qry_eval'].values, df_res['eval_y'].values[::-1], c='black', transform= rot + base)\n",
    "    axHisty.plot(df_FWHM_y['X_fit'], df_FWHM_y['Y_fit'].values[::-1], c='red', linestyle='dotted', transform= rot + base)\n",
    "    # axHisty.plot(df_FWHM_y['X_fit'], tophat(df_FWHM_y['X_fit'], *(tophat_params[1]))[::-1], c='green', linestyle='dotted', transform= rot + base)\n",
    "\n",
    "\n",
    "    axHistx.set_xlim(axScatter.get_xlim())\n",
    "    axHisty.set_ylim(axScatter.get_ylim())\n",
    "\n",
    "    # f.tight_layout()\n",
    "    # df_x_FWHM, df_y_FWHM = getLargestFWHM(k, lim)\n",
    "\n",
    "    # df_x_FWHM.to_csv('{}/df_x_FWHM.csv'.format(directory))\n",
    "    # df_y_FWHM.to_csv('{}/df_y_FWHM.csv'.format(directory))\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    filename =  '{}/{}'.format(directory, runfile)\n",
    "    df_histData.to_csv(f'{filename}_df_histData.csv')\n",
    "    # print(nn)\n",
    "    # plt.savefig(filename + '.eps', dpi=1200)\n",
    "    # plt.savefig(filename + '.svg', dpi=1200)\n",
    "    plt.savefig(filename + '.png', dpi=600)\n",
    "\n",
    "    plt.close('all')\n",
    "    if 'BIDIR' in runfile:\n",
    "        s_type = 'BIDIR'\n",
    "    else:\n",
    "        s_type = 'TD'\n",
    "    # id = re.findall(r'(\\d\\d)\\.', runfile)[0]\n",
    "    df_FWHM['id'] = runfile\n",
    "    df_FWHM['runfile'] = runfile\n",
    "    df_FWHM['run_type'] = s_type\n",
    "    df_output = df_output.append(df_FWHM)\n",
    "    # plt.show()\n",
    "\n",
    "    fname = f'{COMSOL_data_file_path}/plots/2D_histograms_lastTimestep/df_FWHMs.csv'\n",
    "    df_output.to_csv(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
